Instance status:
SystemSetup: Succeeded
UserContainerImagePull: Succeeded
ModelDownload: Succeeded
UserContainerStart: InProgress

Container events:
Kind: Pod, Name: Downloading, Type: Normal, Time: 2026-02-03T15:07:12.429224Z, Message: Start downloading models
Kind: Pod, Name: Pulling, Type: Normal, Time: 2026-02-03T15:07:12.481386Z, Message: Start pulling container image
Kind: Pod, Name: Pulled, Type: Normal, Time: 2026-02-03T15:08:59.082692Z, Message: Container image is pulled successfully
Kind: Pod, Name: Downloaded, Type: Normal, Time: 2026-02-03T15:08:59.082692Z, Message: Models are downloaded successfully
Kind: Pod, Name: Created, Type: Normal, Time: 2026-02-03T15:08:59.136742Z, Message: Created container inference-server
Kind: Pod, Name: Started, Type: Normal, Time: 2026-02-03T15:08:59.191779Z, Message: Started container inference-server

Container logs:
2026-02-03T15:08:59,200055499+00:00 - rsyslog/run 
2026-02-03T15:08:59,206167812+00:00 - nginx/run 
2026-02-03T15:08:59,214296028+00:00 - gunicorn/run 
2026-02-03T15:08:59,215860308+00:00 | gunicorn/run | 
2026-02-03T15:08:59,217426588+00:00 | gunicorn/run | ###############################################
2026-02-03T15:08:59,218824059+00:00 | gunicorn/run | AzureML Container Runtime Information
2026-02-03T15:08:59,220265433+00:00 | gunicorn/run | ###############################################
2026-02-03T15:08:59,221747009+00:00 | gunicorn/run | 
2026-02-03T15:08:59,223375492+00:00 | gunicorn/run | 
2026-02-03T15:08:59,224826167+00:00 | gunicorn/run | PATH environment variable: /azureml-envs/azureml-automl/bin:/usr/local/openmpi/bin:/opt/miniconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
2026-02-03T15:08:59,226327943+00:00 | gunicorn/run | PYTHONPATH environment variable: 
2026-02-03T15:08:59,227752416+00:00 | gunicorn/run | 
/opt/miniconda/lib/python3.10/site-packages/conda/cli/conda_argparse.py:253: FutureWarning: `--root` is deprecated and will be removed in 26.3. Use `--base` instead.
  super().__call__(parser, namespace, values, option_string)
2026-02-03T15:08:59,620724724+00:00 | gunicorn/run | CONDAPATH environment variable: /opt/miniconda


# conda environments:
#
# * -> active
# + -> frozen
                         /azureml-envs/azureml-automl
base                     /opt/miniconda

2026-02-03T15:09:00,071568113+00:00 | gunicorn/run | 
2026-02-03T15:09:00,073291096+00:00 | gunicorn/run | Pip Dependencies (before dynamic installation)

adal==1.2.7
annotated-types==0.7.0
anyio==4.12.0
applicationinsights==0.11.10
arch==5.6.0
argcomplete==3.6.3
asttokens==3.0.1
attrs==25.4.0
azure-common==1.1.28
azure-core==1.36.0
azure-graphrbac==0.61.2
azure-identity==1.17.0
azure-mgmt-authorization==4.0.0
azure-mgmt-containerregistry==14.0.0
azure-mgmt-core==1.6.0
azure-mgmt-keyvault==11.0.0
azure-mgmt-network==30.0.0
azure-mgmt-resource==24.0.0
azure-mgmt-storage==24.0.0
azure-monitor-opentelemetry-exporter==1.0.0b40
azure-storage-blob==12.19.0
azure-storage-queue==12.14.1
azureml-automl-core==1.61.0.post1
azureml-automl-runtime==1.61.0
azureml-core==1.61.0.post1
azureml-dataprep==5.4.2
azureml-dataprep-native==42.1.0
azureml-dataprep-rslex==2.25.2
azureml-dataset-runtime==1.61.0
azureml-defaults==1.61.0
azureml-inference-server-http==1.5.0
azureml-interpret==1.61.0
azureml-mlflow==1.61.0.post1
azureml-pipeline-core==1.61.0
azureml-responsibleai==1.61.0
azureml-telemetry==1.61.0
azureml-train-automl==1.61.0.post1
azureml-train-automl-client==1.61.0.post1
azureml-train-automl-runtime==1.61.0
azureml-train-core==1.61.0
azureml-train-restclients-hyperdrive==1.61.0
azureml-training-tabular==1.61.0
backports.tempfile==1.0
backports.weakref==1.0.post1
bcrypt==5.0.0
blinker==1.9.0
bokeh==2.4.3
boto3==1.42.6
botocore==1.42.6
cachetools==5.5.2
certifi==2025.11.12
cffi==2.0.0
charset-normalizer==3.4.4
click==8.3.1
cloudpickle==2.2.1
cmdstanpy==1.0.4
coloredlogs==15.0.1
comm==0.2.3
contextlib2==21.6.0
contourpy==1.3.2
convertdate==2.4.0
cryptography==45.0.7
cycler==0.12.1
Cython==3.2.2
dask==2023.2.0
databricks-sdk==0.74.0
dataclasses==0.6
debugpy==1.8.18
decorator==5.2.1
Deprecated==1.3.1
dice-ml==0.11
dill==0.3.9
distributed==2023.2.0
distro==1.9.0
docker==7.1.0
dotnetcore2==3.1.23
econml==0.16.0
entrypoints==0.4
ephem==4.2
erroranalysis==0.5.5
exceptiongroup==1.3.1
executing==2.2.1
fairlearn==0.7.0
filelock @ file:///home/conda/feedstock_root/build_artifacts/filelock_1759948064277/work
fire==0.7.1
fixedint==0.1.6
Flask==3.1.2
flask-cors==6.0.1
flatbuffers==25.9.23
fonttools==4.61.0
fsspec @ file:///home/conda/feedstock_root/build_artifacts/fsspec_1764784748144/work
fusepy==3.0.1
gensim==4.3.2
gitdb==4.0.12
GitPython==3.1.45
gmpy2 @ file:///home/conda/feedstock_root/build_artifacts/gmpy2_1762946744175/work
google-auth==2.43.0
gunicorn==23.0.0
holidays==0.86
humanfriendly==10.0
idna==3.11
importlib_metadata==7.2.1
importlib_resources==6.4.0
inference-schema==1.8
interpret-core==0.5.0
interpret_community==0.31.0
ipykernel==7.1.0
ipython==8.37.0
isodate==0.7.2
itsdangerous==2.2.0
jedi==0.19.2
jeepney==0.9.0
Jinja2 @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_jinja2_1764517220/work
jmespath==1.0.1
joblib @ file:///home/conda/feedstock_root/build_artifacts/joblib_1663332044897/work
jsonpickle==4.1.1
jsonschema==4.25.1
jsonschema-specifications==2025.9.1
jupyter_client==8.7.0
jupyter_core==5.9.1
keras2onnx==1.6.0
kiwisolver==1.4.9
knack==0.12.0
lightgbm==4.6.0
llvmlite==0.39.1
locket==1.0.0
LunarCalendar==0.0.9
MarkupSafe @ file:///home/conda/feedstock_root/build_artifacts/markupsafe_1759055168201/work
matplotlib==3.10.7
matplotlib-inline==0.2.1
ml_wrappers==0.5.6
mlflow-skinny==2.15.1
mltable==1.6.3
mpmath @ file:///home/conda/feedstock_root/build_artifacts/mpmath_1733302684489/work
msal==1.34.0
msal-extensions==1.3.1
msgpack==1.1.2
msrest==0.7.1
msrestazure==0.6.4.post1
ndg-httpsclient==0.5.1
nest-asyncio==1.6.0
networkx==2.5
numba==0.56.4
numpy==1.23.5
oauthlib==3.3.1
onnx==1.17.0
onnxconverter-common==1.13.0
onnxmltools==1.12.0
onnxruntime==1.17.3
opentelemetry-api==1.33.0
opentelemetry-sdk==1.33.0
opentelemetry-semantic-conventions==0.54b0
optree @ file:///home/conda/feedstock_root/build_artifacts/optree_1763124233920/work
packaging==24.2
pandas==1.5.3
paramiko==3.5.1
parso==0.8.5
partd==1.4.2
pathspec==0.12.1
patsy==1.0.2
pexpect==4.9.0
pillow==12.0.0
pkginfo==1.12.1.2
platformdirs==4.5.1
pmdarima==1.8.5
prompt_toolkit==3.0.52
property-cached==1.6.4
prophet==1.1.4
protobuf==5.29.5
psutil==5.9.3
ptyprocess==0.7.0
pure_eval==0.2.3
py-cpuinfo==5.0.0
pyarrow==14.0.2
pyasn1==0.6.1
pyasn1_modules==0.4.2
pybind11 @ file:///D:/bld/pybind11-split_1747934313674/work
pybind11_global @ file:///home/conda/feedstock_root/build_artifacts/pybind11-split_1747934270866/work
pycparser==2.23
pydantic==2.11.10
pydantic-settings==2.12.0
pydantic_core==2.33.2
Pygments==2.19.2
PyJWT==2.10.1
PyMeeus==0.5.12
PyNaCl==1.6.1
pyOpenSSL==25.3.0
pyparsing==3.2.5
PySocks==1.7.1
python-dateutil==2.9.0.post0
python-dotenv==1.2.1
pytz==2024.2
PyYAML==6.0.3
pyzmq==27.1.0
raiutils==0.4.2
referencing==0.37.0
requests==2.32.5
requests-oauthlib==2.0.0
responsibleai==0.36.0
rpds-py==0.30.0
rsa==4.9.1
s3transfer==0.16.0
scikit-learn==1.5.1
scipy==1.10.1
SecretStorage==3.5.0
semver==2.13.0
setuptools-git==1.2
shap==0.44.0
six==1.17.0
skl2onnx==1.15.0
sklearn-pandas==1.7.0
slicer==0.0.7
smart-open==6.4.0
smmap==5.0.2
sortedcontainers==2.4.0
sparse==0.17.0
sqlparse==0.5.4
stack-data==0.6.3
starlette==0.50.0
statsmodels==0.13.5
sympy @ file:///home/conda/feedstock_root/build_artifacts/sympy_1745946051654/work
tabulate==0.9.0
tblib==3.2.2
termcolor==3.2.0
threadpoolctl==3.6.0
toolz==1.1.0
torch @ file:///home/conda/feedstock_root/build_artifacts/libtorch_1762089971981/work
tornado==6.5.2
tqdm==4.67.1
traitlets==5.14.3
typing-inspection==0.4.2
typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_typing_extensions_1756220668/work
ujson==5.11.0
urllib3==2.5.0
wcwidth==0.2.14
Werkzeug==3.1.4
wrapt==1.16.0
xgboost==1.5.2
zict==3.0.0
zipp==3.23.0

2026-02-03T15:09:00,534604762+00:00 | gunicorn/run | 
2026-02-03T15:09:00,536340146+00:00 | gunicorn/run | Entry script directory: /var/mlflow_resources/.
2026-02-03T15:09:00,538664558+00:00 | gunicorn/run | 
2026-02-03T15:09:00,540337738+00:00 | gunicorn/run | ###############################################
2026-02-03T15:09:00,541870812+00:00 | gunicorn/run | Dynamic Python Package Installation
2026-02-03T15:09:00,543351883+00:00 | gunicorn/run | ###############################################
2026-02-03T15:09:00,544739949+00:00 | gunicorn/run | 
2026-02-03T15:09:00,546603839+00:00 | gunicorn/run | Dynamic Python package installation is disabled.
2026-02-03T15:09:00,548365324+00:00 | gunicorn/run | 
2026-02-03T15:09:00,549968401+00:00 | gunicorn/run | ###############################################
2026-02-03T15:09:00,551566677+00:00 | gunicorn/run | Checking if the Python package azureml-inference-server-http is installed
2026-02-03T15:09:00,553303561+00:00 | gunicorn/run | ###############################################
2026-02-03T15:09:00,554785232+00:00 | gunicorn/run | 
2026-02-03T15:09:01,277568262+00:00 | gunicorn/run | 
2026-02-03T15:09:01,279457253+00:00 | gunicorn/run | ###############################################
2026-02-03T15:09:01,281508051+00:00 | gunicorn/run | AzureML Inference Server
2026-02-03T15:09:01,283498147+00:00 | gunicorn/run | ###############################################
2026-02-03T15:09:01,285103324+00:00 | gunicorn/run | 
2026-02-03T15:09:01,287129622+00:00 | gunicorn/run | Starting AzureML Inference Server HTTP.
2026-02-03 15:09:01,457 I [16] azmlinfsrv - Loaded logging config from /azureml-envs/azureml-automl/lib/python3.10/site-packages/azureml_inference_server_http/logging.json
2026-02-03 15:09:01,473 I [16] gunicorn.error - Starting gunicorn 23.0.0
2026-02-03 15:09:01,473 I [16] gunicorn.error - Listening at: http://0.0.0.0:31311 (16)
2026-02-03 15:09:01,473 I [16] gunicorn.error - Using worker: sync
2026-02-03 15:09:01,482 I [63] gunicorn.error - Booting worker with pid: 63

Azure ML Inferencing HTTP server v1.5.0


Server Settings
---------------
Entry Script Name: /var/mlflow_resources/mlflow_score_script.py
Model Directory: /var/azureml-app/azureml-models/hf-best-model/3
Config File: None
Worker Count: 1
Worker Timeout (seconds): 300
Server Port: 31311
Health Port: 31311
Application Insights Enabled: false
Application Insights Key: None
Inferencing HTTP server version: azmlinfsrv/1.5.0
CORS for the specified origins: None
Create dedicated endpoint for health: None


Server Routes
---------------
Liveness Probe: GET   127.0.0.1:31311/
Score:          POST  127.0.0.1:31311/score

2026-02-03 15:09:01,598 W [63] azmlinfsrv - Found extra keys in the config file that are not supported by the server.
Extra keys = ['AZUREML_ENTRY_SCRIPT', 'AZUREML_MODEL_DIR', 'HOSTNAME']
Initializing logger
2026-02-03 15:09:01,890 I [63] azmlinfsrv - Starting up app insights client
2026-02-03 15:09:03,002 E [63] azmlinfsrv - Traceback (most recent call last):
  File "/azureml-envs/azureml-automl/lib/python3.10/site-packages/azureml_inference_server_http/server/user_script.py", line 77, in load_script
    main_module_spec.loader.exec_module(user_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/var/mlflow_resources/mlflow_score_script.py", line 160, in <module>
    model = Model.load(model_path)
  File "/azureml-envs/azureml-automl/lib/python3.10/site-packages/mlflow/models/model.py", line 620, in load
    raise MlflowException(
mlflow.exceptions.MlflowException: Could not find an "MLmodel" configuration file at "/var/azureml-app/azureml-models/hf-best-model/3/mlflow-model"

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/azureml-envs/azureml-automl/lib/python3.10/site-packages/azureml_inference_server_http/server/aml_blueprint.py", line 91, in setup
    self.user_script.load_script(config.app_root)
  File "/azureml-envs/azureml-automl/lib/python3.10/site-packages/azureml_inference_server_http/server/user_script.py", line 79, in load_script
    raise UserScriptImportException(ex) from ex
azureml_inference_server_http.server.user_script.UserScriptImportException: Failed to import user script because it raised an unhandled exception

2026-02-03 15:09:03,003 I [63] gunicorn.error - Worker exiting (pid: 63)
2026-02-03 15:09:03,248 E [16] gunicorn.error - Worker (pid:63) exited with code 3
2026-02-03 15:09:03,249 E [16] gunicorn.error - Shutting down: Master
2026-02-03 15:09:03,249 E [16] gunicorn.error - Reason: Worker failed to boot.

Azure ML Inferencing HTTP server v1.5.0


Server Settings
---------------
Entry Script Name: /var/mlflow_resources/mlflow_score_script.py
Model Directory: /var/azureml-app/azureml-models/hf-best-model/3
Config File: None
Worker Count: 1
Worker Timeout (seconds): 300
Server Port: 31311
Health Port: 31311
Application Insights Enabled: false
Application Insights Key: None
Inferencing HTTP server version: azmlinfsrv/1.5.0
CORS for the specified origins: None
Create dedicated endpoint for health: None


Server Routes
---------------
Liveness Probe: GET   127.0.0.1:31311/
Score:          POST  127.0.0.1:31311/score

2026-02-03T15:09:03,294045455+00:00 - gunicorn/finish 3 0
2026-02-03T15:09:03,296373667+00:00 - Exit code 3 is not normal. Killing image.

